{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 'raw' :: MNIST MLP\n",
    "==============================\n",
    "\n",
    "This is a quick illustration of a single-layer network training on the MNIST data.\n",
    "\n",
    "( Credit for the original workbook : Aymeric Damien :: https://github.com/aymericdamien/TensorFlow-Examples )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the MNIST data\n",
    "Put it into useful subsets, and show some of it as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Download the MNIST digits dataset (only if not present locally)\n",
    "import os\n",
    "import urllib.request \n",
    "\n",
    "mnist_data = './data/MNIST' \n",
    "mnist_pkl_gz = mnist_data+'/mnist.pkl.gz'\n",
    "\n",
    "if not os.path.isfile(mnist_pkl_gz):\n",
    "    if not os.path.exists(mnist_data):\n",
    "        os.makedirs(mnist_data)\n",
    "    print(\"Downloading MNIST data file\")\n",
    "    urllib.request.urlretrieve(\n",
    "        'http://deeplearning.net/data/mnist/mnist.pkl.gz', \n",
    "        mnist_pkl_gz)\n",
    "\n",
    "print(\"MNIST data file available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and test splits as numpy arrays\n",
    "train, val, test = pickle.load(gzip.open(mnist_pkl_gz), encoding='iso-8859-1')\n",
    "\n",
    "X_train, y_train = train\n",
    "X_val, y_val = val\n",
    "X_test, y_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The original 28x28 pixel images are flattened into 784 dimensional feature vectors\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the first few examples \n",
    "plt.figure(figsize=(12,3))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(X_train[i].reshape((28, 28)), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input    = 784 # MNIST data input (img shape: 28*28)\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_classes  = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float32\", [None, n_input],   name='x_input')\n",
    "#y = tf.placeholder(\"int32\", [None, n_classes], name='y_target')  # originally, a one-hot label\n",
    "y = tf.placeholder(\"int32\", [ None, ], name='y_target')  # This is the label index instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, layer):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, layer['h1']['weights']), layer['h1']['bias'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, layer['h2']['weights']), layer['h2']['bias'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, layer['out']['weights']) + layer['out']['bias']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "layers=dict(\n",
    "    h1 = { \n",
    "        'weights':tf.Variable(tf.random_normal([n_input, n_hidden_1])),  \n",
    "        'bias'   :tf.Variable(tf.random_normal([n_hidden_1])),  \n",
    "    },\n",
    "    h2 = { \n",
    "        'weights':tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),  \n",
    "        'bias'   :tf.Variable(tf.random_normal([n_hidden_2])),  \n",
    "    },\n",
    "    out = { \n",
    "        'weights':tf.Variable(tf.random_normal([n_hidden_2, n_classes])),  \n",
    "        'bias'   :tf.Variable(tf.random_normal([n_classes])),  \n",
    "    },\n",
    ")\n",
    "    \n",
    "# Construct model\n",
    "logits = multilayer_perceptron(x, layers)\n",
    "#pred = tf.argmax(logits, axis=1)  # being deprecated\n",
    "pred = tf.arg_max(logits, 1)\n",
    "#pred = tf.reshape( tf.arg_max(logits, 1), [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Loss Function\n",
    "So that we can perform Gradient Descent to improve the networks' parameters during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer for the labels (expressed as a onehot encoding)\n",
    "labels = tf.one_hot(indices=y, depth=10)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the training phase\n",
    "learning_rate = 0.001\n",
    "TRAINING_EPOCHS = 10\n",
    "BATCH_SIZE = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Initializer\n",
    "NB: Do this after creating all the variables (including those inplicitly create in, say AdamOptimizer) since this figures out all the variables that need initializing at the point it is defined, apparently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define an 'op' that initializes the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching of Training\n",
    "For efficiency, we operate on data in batches, so that (for instance) a GPU can operate on multiple examples simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll choose a batch size, and calculate the number of batches in an \"epoch\"\n",
    "# (approximately one pass through the data).\n",
    "N_BATCHES = len(X_train) // BATCH_SIZE\n",
    "#N_VAL_BATCHES = len(X_val) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# For training, we want to sample examples at random in small batches\n",
    "def batch_gen(X_, y_, N):\n",
    "    while True:\n",
    "        idx = np.random.choice(len(y_), N)\n",
    "        yield X_[idx], y_[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minibatch generator(s) for the training and validation sets\n",
    "train_batches = batch_gen(X_train, y_train, BATCH_SIZE)\n",
    "#val_batches = batch_gen(X_val, y_val, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try sampling from the batch generator.\n",
    "# Plot an image and corresponding label from the training batcher to verify they match.\n",
    "X_batch, y_batch = next(train_batches)\n",
    "plt.imshow(X_batch[0].reshape((28, 28)), cmap='gray', interpolation='nearest')\n",
    "print(y_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an image and corresponding label from the validation set to verify they match.\n",
    "X_batch, y_batch = X_val, y_val\n",
    "plt.imshow(X_batch[0].reshape((28, 28)), cmap='gray', interpolation='nearest')\n",
    "print(y_batch[0])\n",
    "X_batch.shape, y_batch.shape, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function to check final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))  # with one-hots\n",
    "correct_prediction = tf.equal(pred, tf.cast(y, tf.int64))  # with indices\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the Training...\n",
    "For each epoch, we call the training function N_BATCHES times, accumulating an estimate of the training loss and accuracy.\n",
    "\n",
    "* Then we evaluate the accuracy on the validation set.\n",
    "\n",
    "TODO : Print out the ratio of loss in the validation set vs the training set to help recognize overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)  # Running this 'op' initialises the network weights\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(TRAINING_EPOCHS):\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for _ in range(N_BATCHES):\n",
    "            batch_x, batch_y = next(train_batches)\n",
    "            #print(batch_x.shape, batch_y.shape)\n",
    "            \n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x:batch_x, y:batch_y})\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_cost += c / N_BATCHES\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\",\"{:.2f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    accuracy_, y_, pred_  = sess.run([accuracy, y, pred ], feed_dict={x:X_val[0:10], y:y_val[0:10] })\n",
    "    print(\"Validation Accuracy: %.2f%% for first 10 examples\"  % ( 100. * accuracy_, ))\n",
    "    #print(y_)\n",
    "    #print(pred_)\n",
    "    print(\"Validation Accuracy: %.2f%%\"  % ( 100. * accuracy.eval({ x: X_val,  y: y_val, }),))\n",
    "    \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This proves that when the sessions are done, the network is thrown away...\n",
    "#with tf.Session() as sess:\n",
    "#    accuracy_, y_, pred_  = sess.run([accuracy, y, pred ], feed_dict={x:X_test[0:100], y:y_test[0:100] })\n",
    "#    print(\"Test Accuracy: %.2f%% for first 100 examples\"  % ( 100. * accuracy_, ))\n",
    "#print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Weight Matrix\n",
    "We can retrieve the value of the trained weight matrix from the output layer.\n",
    "\n",
    "It can be interpreted as a collection of images, one per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#weights = l_out.W.get_value()\n",
    "#print(weights.shape)\n",
    "#with tf.Session() as sess:\n",
    "#    accuracy_, y_, pred_  = sess.run([accuracy, y, pred ], feed_dict={x:X_val[0:10], y:y_val[0:10] }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the weight images.  \n",
    "We should expect to recognize similarities to the target images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(12,3))\n",
    "#for i in range(10):\n",
    "#    plt.subplot(1, 10, i+1)\n",
    "#    plt.imshow(weights[:,i].reshape((28, 28)), cmap='gray', interpolation='nearest')\n",
    "#    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
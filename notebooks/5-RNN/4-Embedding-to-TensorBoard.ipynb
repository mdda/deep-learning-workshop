{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def embedding_to_tensorboard(word_vectors, vocab_arr, name):  # name=\"embedding_title\"\n",
    "    #N = 10000 # Number of items (vocab size).\n",
    "    #D = 200 # Dimensionality of the embedding.\n",
    "    #embedding_var = tf.Variable(tf.random_normal([N,D]), name='word_embedding')\n",
    "\n",
    "    embedding_var = tf.Variable(word_vectors, dtype='float32', \n",
    "                                name=name)\n",
    "\n",
    "    # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "    projector_config = projector.ProjectorConfig()\n",
    "\n",
    "    # You can add multiple embeddings. Here we add only one.\n",
    "    embedding = projector_config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "\n",
    "    # Link this tensor to its metadata file (e.g. labels).\n",
    "    LOG_DIR='../../tensorflow.logdir/'\n",
    "    #metadata_file = 'glove_full_50d.words.tsv'\n",
    "    metadata_file = name + '.tsv'\n",
    "    #vocab_list = [ word_embedding.inverse_dictionary[i] \n",
    "    #               for i in range(len( word_embedding.inverse_dictionary )) ]\n",
    "    with open(os.path.join(LOG_DIR, metadata_file), 'wt') as metadata:\n",
    "        metadata.writelines(\"%s\\n\" % w for w in vocab_arr)\n",
    "\n",
    "    embedding.metadata_path = os.path.join(os.getcwd(), LOG_DIR, metadata_file)\n",
    "\n",
    "    # Use the same LOG_DIR where you stored your checkpoint.\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "\n",
    "    # The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n",
    "    # read this file during startup.\n",
    "    projector.visualize_embeddings(summary_writer, projector_config)\n",
    "\n",
    "    saver = tf.train.Saver([embedding_var])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.save(sess, os.path.join(LOG_DIR, metadata_file+'.ckpt'))\n",
    "        \n",
    "    print(\"Look at the embedding in TensorBoard : http://localhost:8081/#projector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glove\n",
    "\n",
    "glove_dir = './data/RNN/'\n",
    "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
    "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
    "\n",
    "glove_embedding = glove.Glove.load_stanford( glove_100k_50d_path )\n",
    "#glove_embedding.word_vectors.shape\n",
    "\n",
    "embedding_to_tensorboard( glove_embedding.word_vectors, \n",
    "                          [ glove_embedding.inverse_dictionary[i] \n",
    "                            for i in range(len( glove_embedding.inverse_dictionary )) ],\n",
    "                          'wiki_embedding' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('poc3-spotify/st-ner.model.txt', \"r\", encoding='utf-8', errors='ignore') as fner:\n",
    "    ner_len, ner_dim = [ int(i) for i in fner.readline().strip().split(' ')]\n",
    "    print(ner_len, ner_dim)\n",
    "    \n",
    "    valid_lines = [ l for l in fner.readlines() if 'jpg' not in l ]\n",
    "    #valid_lines = fner.readlines()\n",
    "\n",
    "    ner_vec = np.zeros( (len(valid_lines), ner_dim) )\n",
    "    ner_str_arr = []\n",
    "    for idx, l in enumerate( valid_lines ):\n",
    "      l_str = l.strip().split(' ')\n",
    "      ner_str_arr.append( l_str[0] )\n",
    "      ner_vec[idx] = np.array( [ float(x) for x in l_str[1:] ]) \n",
    "\n",
    "# Normalise all vector rows to L2=1\n",
    "ner_vec = ner_vec / np.linalg.norm(ner_vec, axis=1, keepdims=True)\n",
    "ner_str_to_idx = { s:idx for idx, s in enumerate(ner_str_arr) } \n",
    "\n",
    "print( ner_vec.shape, len(ner_str_arr), )\n",
    "embedding_to_tensorboard( ner_vec, \n",
    "                          ner_str_arr, \n",
    "                          'NER_embedding' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Purposing a Pretrained Network\n",
    "\n",
    "Since a large CNN is very time-consuming to train (even on a GPU), and requires huge amounts of data, is there any way to use a pre-calculated one instead of retraining the whole thing from scratch?\n",
    "\n",
    "This notebook shows how this can be done.  And it works surprisingly well.\n",
    "\n",
    "\n",
    "##  How do we classify images with untrained classes?\n",
    "\n",
    "This notebook extracts a vector representation of a set of images using a CNN created by Google and pretrained on ImageNet.  It then builds a 'simple SVM classifier', allowing new images can be classified directly.  No retraining of the original CNN is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "#from tensorflow.python.keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "CLASS_DIR='./images/cars'\n",
    "#CLASS_DIR='./images/seefood'  # for HotDog vs NotHotDog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Keras Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/applications/\n",
    "from tensorflow.python.keras.preprocessing import image as keras_preprocessing_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Choices\n",
    "\n",
    "![Architectures](../../images/presentation/Architecture_performance-vs-size_620x456.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASNet cell structure\n",
    "\n",
    "![NASNET cell](../../images/presentation/NASNet-Cell_976x579.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure we have the model loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow.python.keras.applications.nasnet import NASNetLarge, preprocess_input\n",
    "#model = NASNetLarge(weights='imagenet', include_top=False)  # 343,608,736\n",
    "\n",
    "from tensorflow.python.keras.applications.nasnet import NASNetMobile, preprocess_input, decode_predictions\n",
    "\n",
    "model_imagenet = NASNetMobile(weights='imagenet', include_top=True)   # 24,226,656 bytes\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model and select layers we need - the features are taken from the final network layer, before the softmax nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_input(model, img_path):\n",
    "    target_size=model.input_shape[1:]\n",
    "    img = keras_preprocessing_image.load_img(img_path, target_size=target_size)\n",
    "    \n",
    "    x = keras_preprocessing_image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_prediction(img_path, top=5):\n",
    "    x = image_to_input(model_imagenet, img_path)\n",
    "    preds = model_imagenet.predict(x)\n",
    "    predictions = decode_predictions(preds, top=top)\n",
    "    return predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './images/cat-with-tongue_224x224.jpg'\n",
    "im = plt.imread(img_path)\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "for t in get_single_prediction(img_path):\n",
    "    print(\"%6.2f %s\" % (t[2],t[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = './images/'\n",
    "\n",
    "image_files = [ os.path.join(image_dir, f) for f in os.listdir(image_dir) \n",
    "                 if (f.lower().endswith('png') or f.lower().endswith('jpg')) and f!='logo.png' ]\n",
    "\n",
    "t0 = time.time()\n",
    "for i, f in enumerate(image_files):\n",
    "    im = plt.imread(f)\n",
    "    if not (im.shape[0]==224 and im.shape[1]==224):\n",
    "        continue\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(im.astype('uint8'))\n",
    "    \n",
    "    top5 = get_single_prediction(f)\n",
    "    for n, (id,label,prob) in enumerate(top5):\n",
    "        plt.text(350, 50 + n * 25, '{}. {}'.format(n+1, label), fontsize=14)\n",
    "    plt.axis('off')\n",
    "        \n",
    "print(\"DONE : %6.2f seconds each\" %(float(time.time() - t0)/len(image_files),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imagenet=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Transfer Learning\n",
    "\n",
    "Now, we'll work with the layer 'just before' the final (ImageNet) classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logits   = NASNetMobile(weights='imagenet', include_top=False)  # 19,993,200 bytes\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Use the Network to create 'features' for the training images\n",
    "\n",
    "Now go through the input images and feature-ize them at the 'logit level' according to the pretrained network.\n",
    "\n",
    "<!-- [Logits vs the softmax probabilities](images/presentation/softmax-layer-generic_676x327.png) !-->\n",
    "\n",
    "![Network Picture](images/presentation/commerce-network_631x540.png)\n",
    "\n",
    "NB: The pretraining was done on ImageNet - there wasn't anything specific to the recognition task we're doing here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the network layout graph on TensorBoard\n",
    "\n",
    "This isn't very informative, since the CNN graph is pretty complex..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#writer = tf.summary.FileWriter(logdir='../tensorflow.logdir/', graph=tf.get_default_graph())\n",
    "#writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handy cropping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def crop_middle_square_area(np_image):\n",
    "    h, w, _ = np_image.shape\n",
    "    h = int(h/2)\n",
    "    w = int(w/2)\n",
    "    if h>w:\n",
    "        return np_image[ h-w:h+w, : ]\n",
    "    return np_image[ :, w-h:w+h ]    \n",
    "im_sq = crop_middle_square_area(im)\n",
    "im_sq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits_from_non_top(np_logits):\n",
    "    # ~ average pooling\n",
    "    #return np_logits[0].sum(axis=0).sum(axis=0)\n",
    "    \n",
    "    # ~ max-pooling\n",
    "    return np_logits[0].max(axis=0).max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use folder names to imply classes for Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = sorted( [ d for d in os.listdir(CLASS_DIR) if os.path.isdir(os.path.join(CLASS_DIR, d)) ] )\n",
    "classes # Sorted for for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = dict(filepath=[], features=[], target=[])\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for class_i, directory in enumerate(classes):\n",
    "    for filename in os.listdir(os.path.join(CLASS_DIR, directory)):\n",
    "        filepath = os.path.join(CLASS_DIR, directory, filename)\n",
    "        if os.path.isdir(filepath): continue\n",
    "\n",
    "        im = plt.imread(filepath)\n",
    "        im_sq = crop_middle_square_area(im)\n",
    "\n",
    "        x = image_to_input(model_logits, filepath)\n",
    "        np_logits = model_logits.predict(x)  # Shape = 1x7x7x1056\n",
    "        \n",
    "        np_logits_pooled = get_logits_from_non_top( np_logits )\n",
    "        \n",
    "        train['filepath'].append(filepath)\n",
    "        train['features'].append(np_logits_pooled)\n",
    "        train['target'].append( class_i )\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(im_sq.astype('uint8'))\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.text(2*320, 50, '{}'.format(filename), fontsize=14)\n",
    "        plt.text(2*320, 80, 'Train as class \"{}\"'.format(directory), fontsize=12)\n",
    "\n",
    "print(\"DONE : %6.2f seconds each\" %(float(time.time() - t0)/len(train),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build an SVM model over the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(train['features'], train['target']) # learn from the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Use the SVM model to classify the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_image_files = [f for f in os.listdir(CLASS_DIR) if not os.path.isdir(os.path.join(CLASS_DIR, f))]\n",
    "\n",
    "t0 = time.time()\n",
    "for filename in sorted(test_image_files):\n",
    "    filepath = os.path.join(CLASS_DIR, filename)\n",
    "    im = plt.imread(filepath)\n",
    "    im_sq = crop_middle_square_area(im)\n",
    "\n",
    "    # This is two ops : one merely loads the image from numpy, \n",
    "    #   the other runs the network to get the class probabilities\n",
    "    x = image_to_input(model_logits, filepath)\n",
    "    np_logits = model_logits.predict(x)  # Shape = 1x7x7x1056\n",
    "\n",
    "    np_logits_pooled = get_logits_from_non_top( np_logits )\n",
    "\n",
    "    prediction_i = classifier.predict([ np_logits_pooled ])\n",
    "    decision     = classifier.decision_function([ np_logits_pooled ])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(im_sq.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "\n",
    "    prediction = classes[ prediction_i[0] ]\n",
    "\n",
    "    plt.text(2*320, 50, '{} : Distance from boundary = {:5.2f}'.format(prediction, decision[0]), fontsize=20)\n",
    "    plt.text(2*320, 75, '{}'.format(filename), fontsize=14)\n",
    "\n",
    "print(\"DONE : %6.2f seconds each\" %(float(time.time() - t0)/len(test_image_files),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Exercise : Try your own ideas\n",
    "\n",
    "The whole training regime here is based on the way the image directories are structured.  So building your own example shouldn't be very difficult.\n",
    "\n",
    "Suppose you wanted to classify pianos into Upright and Grand : \n",
    "\n",
    "*  Create a ```pianos``` directory and point the ```CLASS_DIR``` variable at it\n",
    "*  Within the ```pianos``` directory, create subdirectories for each of the classes (i.e. ```Upright``` and ```Grand```).  The directory names will be used as the class labels\n",
    "*  Inside the class directories, put a 'bunch' of positive examples of the respective classes - these can be images in any reasonable format, of any size (no smaller than 224x224).\n",
    "   +  The images will be automatically resized so that their smallest dimension is 224, and then a square 'crop' area taken from their centers (since ImageNet networks are typically tuned to answering on 224x224 images)\n",
    "*  Test images should be put in the ```pianos``` directory itelf (which is logical, since we don't *know* their classes yet)\n",
    "\n",
    "Finally, re-run everything - checking that the training images are read in correctly, that there are no errors along the way, and that (finally) the class predictions on the test set come out as expected.\n",
    "\n",
    "If/when it works - please let everyone know : We can add that as an example for next time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}